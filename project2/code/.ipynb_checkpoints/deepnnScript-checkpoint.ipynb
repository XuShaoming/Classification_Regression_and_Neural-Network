{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "# Add more hidden layers to create deeper networks\n",
    "# Remember to connect the final hidden layer to the out_layer\n",
    "def create_multilayer_perceptron():\n",
    "    # Network Parameters\n",
    "    n_hidden_1 = 256  # 1st layer number of features\n",
    "    n_hidden_2 = 256  # 2nd layer number of features\n",
    "    n_input = 2376  # data input\n",
    "    n_classes = 2\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(\"float\", [None, n_input])\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer,x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multilayer_perceptron_one_layer():\n",
    "    # Network Parameters\n",
    "    n_hidden_1 = 256  # 1st layer number of features\n",
    "    n_input = 2376  # data input\n",
    "    n_classes = 2\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    print(\"hidden layer numbers: \" + str(1))\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(\"float\", [None, n_input])\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_1, weights['out']) + biases['out']\n",
    "    return out_layer,x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "# Add more hidden layers to create deeper networks\n",
    "# Remember to connect the final hidden layer to the out_layer\n",
    "def create_multilayer_perceptron_4_layer():\n",
    "    # Network Parameters\n",
    "    n_hidden_1 = 256  # 1st layer number of features\n",
    "    n_hidden_2 = 256  # 2nd layer number of features\n",
    "    n_hidden_3 = 256  # 2nd layer number of features\n",
    "    n_hidden_4 = 256  # 2nd layer number of features\n",
    "    n_input = 2376  # data input\n",
    "    n_classes = 2\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "        'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "        'h4': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_4])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_4, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "        'b4': tf.Variable(tf.random_normal([n_hidden_4])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    print(\"hidden layer numbers: \" + str(4))\n",
    "    \n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(\"float\", [None, n_input])\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    layer_3 = tf.nn.relu(layer_3)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])\n",
    "    layer_4 = tf.nn.relu(layer_4)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_4, weights['out']) + biases['out']\n",
    "    return out_layer,x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "# Add more hidden layers to create deeper networks\n",
    "# Remember to connect the final hidden layer to the out_layer\n",
    "def create_multilayer_perceptron_8_layer():\n",
    "    # Network Parameters\n",
    "    n_hidden_1 = 256  # 1st layer number of features\n",
    "    n_hidden_2 = 256  # 2nd layer number of features\n",
    "    n_hidden_3 = 256  # 3nd layer number of features\n",
    "    n_hidden_4 = 256  # 4nd layer number of features\n",
    "    n_hidden_5 = 256  # 5st layer number of features\n",
    "    n_hidden_6 = 256  # 6nd layer number of features\n",
    "    n_hidden_7 = 256  # 7nd layer number of features\n",
    "    n_hidden_8 = 256  # 8nd layer number of features\n",
    "    n_input = 2376  # data input\n",
    "    n_classes = 2\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "        'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "        'h4': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_4])),\n",
    "        'h5': tf.Variable(tf.random_normal([n_hidden_4, n_hidden_5])),\n",
    "        'h6': tf.Variable(tf.random_normal([n_hidden_5, n_hidden_6])),\n",
    "        'h7': tf.Variable(tf.random_normal([n_hidden_6, n_hidden_7])),\n",
    "        'h8': tf.Variable(tf.random_normal([n_hidden_7, n_hidden_8])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_8, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "        'b4': tf.Variable(tf.random_normal([n_hidden_4])),\n",
    "        'b5': tf.Variable(tf.random_normal([n_hidden_5])),\n",
    "        'b6': tf.Variable(tf.random_normal([n_hidden_6])),\n",
    "        'b7': tf.Variable(tf.random_normal([n_hidden_7])),\n",
    "        'b8': tf.Variable(tf.random_normal([n_hidden_8])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    print(\"hidden layer numbers: \" + str(8))\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(\"float\", [None, n_input])\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    layer_3 = tf.nn.relu(layer_3)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])\n",
    "    layer_4 = tf.nn.relu(layer_4)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_5 = tf.add(tf.matmul(layer_4, weights['h5']), biases['b5'])\n",
    "    layer_5 = tf.nn.relu(layer_5)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_6 = tf.add(tf.matmul(layer_5, weights['h6']), biases['b6'])\n",
    "    layer_6 = tf.nn.relu(layer_6)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_7 = tf.add(tf.matmul(layer_6, weights['h7']), biases['b7'])\n",
    "    layer_7 = tf.nn.relu(layer_7)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_8 = tf.add(tf.matmul(layer_7, weights['h8']), biases['b8'])\n",
    "    layer_8 = tf.nn.relu(layer_8)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_8, weights['out']) + biases['out']\n",
    "    return out_layer,x,y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
